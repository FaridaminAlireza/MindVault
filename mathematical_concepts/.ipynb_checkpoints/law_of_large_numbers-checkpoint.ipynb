{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404397c6-5c29-44e9-a860-30a13c2e0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# ! pip3 install scipy\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6feac6d6-d150-47c3-9869-e65b2ef5a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97140268 -0.23406852 -0.10130953 -0.89579478  1.07827426  0.24761632\n",
      "  0.10037077  0.67288597  0.03296816  1.02691856]\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "std_dev = 1\n",
    "\n",
    "data = np.random.normal(mean, std_dev, 1000)\n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d87ac-7437-4614-9166-b05d61b25c29",
   "metadata": {},
   "source": [
    "<h4>The law of large numbers</h4>\n",
    "The law of large numbers is a fundamental theorem in probability that describes the result of performing the same experiment a large number of times. It states that as the number of trials increases, the sample average of the outcomes will converge to the expected value.\n",
    "\n",
    "\n",
    "\n",
    "$$\\overline{X}_n = \\frac{X_1+ X_2 + ... + X_n}{n}$$\n",
    "based on the samples $X_1, X_2,..., X_n$ from a probablitiy distribution with expectation $\\mu$.\n",
    "\n",
    "<h4>Weak law of large numbers</h4>\n",
    "Given a sequence of independent and identically distributed (i.i.d.) random variables with finite expected value $\\mu$, the sample\n",
    "mean $\\overline{X}_n$ converages to\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\text{ } P(|\\overline{X}_n - \\mu| > \\epsilon) = 0 $$\n",
    "<br><br>\n",
    "This means that for large n, the probability of the sample mean deviating from the true mean by any fixed amount $\\epsilon$ becomes very small.\n",
    "\n",
    "<h4>Strong law of large numbers (SLLN)</h4>\n",
    "The sample mean $\\overline{X}_n$ converages almost surely to $\\mu$\n",
    "$$\\lim_{n \\to \\infty} \\text{ } P(\\overline{X}_n = \\mu) = 1 $$\n",
    "\n",
    "It gaurantees that the sample mean will almost surely equal the expected value in the long run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad76b7-8ba6-4252-916f-f5d4b04e7301",
   "metadata": {},
   "source": [
    "<h4>Empirical Distribution Function (EDF)</h4>\n",
    "\n",
    "It's a statistical function used to estimate the cummulative distribution function (CDF) of a sample of data. It provides a non-parametric estimate of the underlying distribution from which the sample was drawn.\n",
    "\n",
    "$$F_n(x) = \\frac{ \\text{ number of elements in the dateset } <= x}{n} $$ </h4>\n",
    "<p> In fact, there is a one-to-one relation between histogram and empirical distribution function. The area under the histogram on a single bin is equal to the relative frequency of elements that lie in that bin, which is also equal to the increase of $F_n$ on that bin.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ce4de-1169-4840-bdbc-48474fa16f3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Given a sample of n independent and identically distributed (i.i.d.) random variables $X_1, X_2,..., X_n$, the empirical distribution function F_n(x) is defined as \n",
    " \n",
    "$$F_n(x) = \\frac{1}{n}  \\sum_{i=1}^n 1_{ X_i <= x}$$\n",
    "\n",
    "Where $ 1_{ X_i <= x}$ is an indicator function that equals 1 if  $X_i <x$ and 0 otherwise.\n",
    "$F_n(x)$ represents the proportion of sample points less than or equal to x.\n",
    "\n",
    "<h4>Properties</h4>\n",
    "\n",
    "- Step Function: The EDF is a step function that increases by $1/n$ at each sample point.\n",
    "- Convergence: $F_n(x)$ converages uniformly to the true cumulative distribution function F(x) as $n \\to \\infty$\n",
    "- For most realizations of the random sample the empirical distribution function $F_n$ is close to F: $$F_n(a) \\approx F(a) $$\n",
    "- Law of Large Numbers: The EDF satisfies the Law of Large Numbers, meaning it approaches the true distribution as more data is collected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9cacfd-6c80-4824-b71f-2df98e6f49cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical distribution function value for 0: 0.498\n"
     ]
    }
   ],
   "source": [
    "def empirical_distribution_function(data, value):\n",
    "\n",
    "    sorted_data = np.sort(data)\n",
    "    \n",
    "    # Count the number of data points less than or equal to the given value\n",
    "    count = np.sum(sorted_data <= value)\n",
    "    \n",
    "    # Compute the EDF\n",
    "    edf = count / len(sorted_data)\n",
    "    \n",
    "    return edf\n",
    "\n",
    "\n",
    "data = np.random.normal(0, 1, 1000)  \n",
    "value = 0  # Example value\n",
    "edf_value = empirical_distribution_function(data, value)\n",
    "print(f\"Empirical distribution function value for {value}: {edf_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0576c5-54b6-4374-9526-580b4ed9ad3f",
   "metadata": {},
   "source": [
    "<h4>Identically distributed and Independent Random Variables</h4>\n",
    "Identically distributed: Each $X_i$ should come from the same underlying distribution.\n",
    "\n",
    "Independent: No sample value should influence another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d2879-ecaa-4b41-b2ed-799b757a2253",
   "metadata": {},
   "source": [
    "<h3> The inverse cumulative distribution function (CDF) </h3> It's a function that \"undoes\" \n",
    "the cumulative distribution function. Given a probability  p, it returns the value\n",
    "of the random variable such that the probability of the random variable being less than\n",
    "or equal to that value is p. In mathematical terms, for a random variable X with\n",
    "cumulative distribution function $F(x)$, the inverse CDF, denoted as $F^{âˆ’1}(p)$, is such that:\n",
    "\n",
    "$$F^{-1}(p) =x  \\text{ if  } F(x) = p$$\n",
    "\n",
    "In simpler terms, if you have a probability p, the inverse CDF gives you the value \n",
    "x such that F(x)=p.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b7a2d6-3255-449d-9d8c-f08a547d3639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDF at 1.5: 0.9331927987311419\n",
      "CDF at 0: 0.5\n"
     ]
    }
   ],
   "source": [
    "def normal_cdf(x, mu, sigma):\n",
    "    # x: The value(s) at which to compute the CDF.\n",
    "    return norm.cdf(x, mu, sigma)\n",
    "\n",
    "\n",
    "mu = 0\n",
    "sigma = 1\n",
    "x = 1.5\n",
    "cdf_value = normal_cdf(x, mu, sigma)\n",
    "print(f\"CDF at {x}: {cdf_value}\")\n",
    "x = 0\n",
    "cdf_value = normal_cdf(x, mu, sigma)\n",
    "print(f\"CDF at {x}: {cdf_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d3a29a-2a7b-4ff4-a226-3117e4f5113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value corresponding to the probability 0.5: 0.0\n",
      "The value corresponding to the probability 0.9331927987311419: 1.4999999999999996\n"
     ]
    }
   ],
   "source": [
    "def inverse_normal_cdf(p, mu, sigma):\n",
    "    #  percent-point function (ppf) aka inverse CDF\n",
    "    return norm.ppf(p, mu, sigma)\n",
    "\n",
    "mu = 0\n",
    "sigma = 1\n",
    "p = 0.5\n",
    "x = inverse_normal_cdf(p, mu, sigma)\n",
    "print(f\"The value corresponding to the probability {p}: {x}\")\n",
    "p = 0.9331927987311419\n",
    "x = inverse_normal_cdf(p, mu, sigma)\n",
    "print(f\"The value corresponding to the probability {p}: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae9ae0-fc77-4bc0-a5ce-811f7b9c11d8",
   "metadata": {},
   "source": [
    "<h4>Non-paramteric estimates </h4>\n",
    "By lack of konwledge on a phenomenon we prefer not to specify a particular parametric type of distribution, and we model our given data set as the realization of a random sample of size N from a continous probablity distribution.\n",
    "<br><br>\n",
    "The kernel density estimate and the emprical distribution function of the dataset approximate the probability density function f and the distribution function F of the distribution. From the resulted graphs, we can understand whether they resemble the probability density function or distriubtion function of any of the familiar parametric distriubtions.\n",
    "<br><br>\n",
    "Instead of viewing the two graphs (kernel density estimate and the empirical distribution function) only as graphical summeries of the data, we can also use both curves as estimates for f and F. We estimate the model probability density f by means of the kernel density estimate and the model distribution fucntion F by means of the empirical distribution function. \n",
    "<br><br>\n",
    "Since neither estimates assumes a particular paramteric model, they are called nonparamteric estimates. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
